#!/usr/bin/env python
# coding: utf-8

# # Day 4 In-Class Assignment: Multidimensional Regression
# 
# <img src="https://assets.amuniversal.com/7f343ac0870a01332a13005056a9545d" width=400px>

# ---
# ## Goals
# * read in yet another data format
# * work with bad data
# * multiple regression with 

# ## 0. Our imports

# In[1]:


import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt


# ## 1. Working with example data 
# 
# We are going to work with some data generated by U.N.E.S.C.O. (United Nations Education, Scientific, and Cultural Organization) and data they collected relating to poverty and inequality in the world. There are two files you need to do the work:
# 
# - `poverty.dat` which is the data file itself
# - `poverty.txt` which describes the data columns as **fixed width column** data. That is, this file describes the columns of the data for each category. For example, the data in columns 1-6 of `poverty.dat` contain the "live birth rates per 1,000 population".
# 
# Conveniently there is a fixed width column pandas data reader. Look it up and read in the data.
# 
# Again, no column headers on this so look at the `poverty.txt` file and give them short but useful names.
# 
# <font size=8 color="#009600">&#9998;</font> Do This - Read in the data into a DataFrame and print the `head()`.

# In[2]:


## your code here


# ### 1.1 Type of the data
# 
# Now look at the `.dtypes` of your DataFrame and describe to me anything unusual. Can you explain why? Please write below. Don't skimp and look ahead, think about it and answer! We'll all wait <font size=6 color="#009600">&#9202;</font>

# <font size=8 color="#009600">&#9998;</font> Answer here

# ### 1.2 Handling missing data - Imputation
# 
# Let's face it, sometimes data is bad. Values are not recorded, or are mis-recorded, or are so far out of expectation that you expect there is something wrong. On the other hand, just **changing** the data seems like cheating. We have to work with what we have, and if we have to make changes it would be good to do that programmatically so that it is recorded for others to see. 
# 
# The process of <a href="https://en.wikipedia.org/wiki/Imputation_(statistics)"> imputation </a> is the statistical replacement of missing/bad data with substitute values. We have that problem here. In the **GNP** column some of the values are set to " \* " indicating missing data. When pandas read in the column the only type that makes sense for both characters and numbers is a string. Therefore it set the type to `object` instead of the expected `int64` or `float64`.
# 
# #### Using numpy.nan
# 
# For better or worse, pandas assumes that "bad values" are marked in the data as numpy **NaN**. NaN is short for "Not a Number". If they are so marked we have access to some of the imputation methods, replacing NaN with various values (mean, median, specific value, etc.). 
# 
# There are two ways to do this:
# 1. you can do a `.replace` on the column using a dictionary of {value to replace : new value, ...} pairs. Remember to save the result. This leaves you with changing the column type using `.astype` but you will have convert to a float, perhaps `"float64"` would be good. You cannot convert a `np.nan` to an integer but you can to a float.
# 2. you can convert the everything that can be converted to a number using `.to_numeric`. Conveniently if it can't do the conversion on a particular value it is set to a `np.nan`
# 
# <font size=8 color="#009600">&#9998;</font> Do This - Convert the missing entries in the GNP column to `np.nan` and show the head of your modified DataFrame. Also print the `dtypes` to show that the column has change type.

# In[3]:


## your code here


# #### Changing numpy.nan
# 
# Now that "bad values" are marked as `numpy.nan`, we can use the DataFrame method `fillna` to change those values. For example:

# In[4]:


## Uncomment to run

# poverty_df["GNP"].fillna(0)


# returns a new DataFrame where all the `np.nan` in the GNP column are replaced with 0. You can do other things are well, for example:

# In[5]:


## Uncomment to run
# poverty_df["GNP"].fillna(poverty_df["GNP"].mean() )
# poverty_df.fillna({"GNP": poverty_df["GNP"].mean() })


# The first version changes any `np.nan` in the `GNP` column to be the mean of the column. The second takes a dictionary where the the key of the dictionary is the column to change and the value is what to replace the `np.nan` with. Note you could replace with other values like: median, min, max, or some other fixed value.
# 
# Remember that all of these examples return either a new Series (when working with just a column) or a DataFrame (if working with the entire element). Nothing is changed in the original unless you assign the result or use `inplace=True` in the call.
# 
# Finally, if you decide that the right thing to do is **remove** any row with a `np.nan` value, we can use the `.dropna` method of DataFrames as shown below:

# In[6]:


## Uncomment to run
# len(poverty_df)
# poverty_df_dropped = poverty_df.dropna()
# print(len(poverty_df), len(poverty_df_dropped))


# #### What do you think
# 
# In the cell below, discuss with your group what you think is the best thing to do with the "bad values" in the DataFrame given the discussion above. Write your result below.

# <font size=8 color="#009600">&#9998;</font> Answer here

# ## 2. Multiple Regression
# 
# In the past, we have limited ourselves to either a single feature or, in the pre-class, doing polynomial regression with other features we created. However, we can just as easily use all, or some combination of all, the features available to make a OLS model. The question is, is it a good idea to just use all the possible features available to make a model?
# 
# Please discuss that idea with your group and record your answer below.

# <font size=8 color="#009600">&#9998;</font> Answer here

# ### 2.1 Infant Mortality model
# 
# Using the U.N.E.S.C.O. data, we can make a model of "Infant Mortality" as the dependent variable against all the other available features. As a hint, an easy way to do this is the make the `sm.OLS` model with  "Infant Mortality" as the first argument (the dependent variable) and then the entire DataFrame where "Infant Mortality is dropped as the second argument. **You should also drop the "Country" column as unique strings don't play well in basic linear models.**
# 
# <font size=8 color="#009600">&#9998;</font> Do This - Make an OLS model that predicts "Infant Mortality" using the other variables (dropping the "Country" column) and print the `.summary` of that process. 

# In[7]:


# your code here


# There are several interesting things about this `.summary`. Let's start with things you have seen before. Look for the adjusted $R^2$ statistic. What does this adjusted $R^2$ tell you about how well your model fits your data?

# <font size=8 color="#009600">&#9998;</font> Answer here

# Now, let's look at something new, the "P" values associated with the features used in the model. [P values](https://en.wikipedia.org/wiki/P-value) are used widely in statisical testing to judge if a result is statistically significant. Those P values that are 0 (or typically less 0.05) indicate a feature that is "significant" in its ability to predict the dependent variable. Those closer, but not equal to, 0 are less significant. It depends on what you did with your "bad values", but list below the top three features and the overall Adjusted R-squared using all the features.

# <font size=8 color="#009600">&#9998;</font> Answer here

# ### 2.2 A "reduced" model using only the "significant" features
# 
# Modeling data is as much a craft as it is a science. We often seek the simplest models that explain or data well because they are typically more interpretable, easier to explain, and provide the information on the main influences of the system we are studying. There are reasons we might want a more complex model to capture the details and the nuance of the system. But for the U.N.E.S.C.O. data that we have, we are likely able to capture most of the system using a smaller number of features. _These ideas are related to the pre-class modeling you did with increasingly higher powers of `x`._
# 
# <font size=8 color="#009600">&#9998;</font> Do This - Redo the model with only the top three features you found above vs "Infant Mortality". Print the summary.

# In[8]:


# your code here


# Review this model and the one you constructed earlier in the notebook. Reporthow the Adjusted R-squared value changed from using only the top three vs using all the available features. How well does this reduced model appear to fit your data?

# <font size=8 color="#009600">&#9998;</font> Answer here

# ## 3. Visualization - How well does our model fit our data?
# 
# We have been checking how our models fit our data using both plots of the fitted values and the residuals. These plots are generated from the information stored in [various attributes of the OLS results object](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.html). We will continue to use the top two plots from `.graphics.plot_regress_exog` to investigate our fits. But you could also construct the plots directly using the attributes of the OLD results object. Note that you will need one plot for each feature in the model as each figure is only produced for a given choice of feature.
# 
# <font size=8 color="#009600">&#9998;</font> Do This - Create three `.graphics.plot_regress_exog` figures, one for each of the features in your model. Pay special attention to the top two plots: the fitted values figure and the residual plot.

# In[9]:


# your code here


# Based on these figures, how well does it appear your reduced model fit your data? Do you have any concerns about the distribution of the residuals?

# <font size=8 color="#009600">&#9998;</font> Answer here
